####################### Data Analysis in Apache Hadoop #######################

Concept:
Performing data analysis using Map Reduce model of Apache Hadoop. 

Implementation:
* Activity 1: Apache Hadoop is used to get a word count of huge number of tweets collected using Twitter API.
* Activity 2: Co-occuring words are calculated from the tweets dataset using pairs and stripe approaches of Apache Hadoop.
* Activity 3: Word count algorithm is used on a huge data set of Latin text files which are lemmatized using a reference file.
* Activity 4: Word Co-occurence is calculated for Latin text data set for 2-Gram and 3-Gram Models.
* Please refer the following project description for in depth details of the activities: https://github.com/Sumedh0192/Data-Mining-and-Computing/blob/master/Data-Processing-in-Hadoop/Project-Description.pdf

Setup:
* Apache Hadoop should be hosted and Hadoop file system should be in place.
* The jar files along with the support files should be moved to the Hadoop file system for execution.
* Individual Activity folders contains all the required files to implement the respective models.
* Each Activity has a Read me file with details of execution and expected output.
